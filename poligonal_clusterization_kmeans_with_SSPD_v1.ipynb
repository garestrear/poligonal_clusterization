{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.16"
    },
    "colab": {
      "name": "poligonal_clusterization_kmeans_with_SSPD_v1.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/garestrear/poligonal_clusterization/blob/master/poligonal_clusterization_kmeans_with_SSPD_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgnyZwWXY86V",
        "colab_type": "text"
      },
      "source": [
        "# Poligonal clusterization using kmeans with SSPD distance.\n",
        "\n",
        "By Gustavo Restrepo. June 2020\n",
        "##**Problem:**\n",
        "\n",
        "We want to clusterizate a set of poligonal curves. All of this are standarized in such way that the first point is always (0,0) and the end point is (1,0). \n",
        "\n",
        "## **Principal Tools:**\n",
        "### **1. kmeans**\n",
        "K-Means is a very simple clustering algorithm (clustering belongs to unsupervised learning). Given a fixed number of clusters and an input dataset the algorithm tries to partition the data into clusters such that the clusters have high intra-class similarity and low inter-class similarity. Here we uses an easy and understandable implementation made for bantak (Git-Hub account of the code author).\n",
        "\n",
        "#### Algorithm\n",
        "\n",
        "1. Initialize the cluster centers, either randomly within the range of the input data or (recommended) with some of the existing training examples\n",
        "\n",
        "2. Until convergence  \n",
        "\n",
        "   2.1. Assign each datapoint to the closest cluster. The distance between a point and cluster center is measured using diferents distances.  \n",
        "\n",
        "   2.2. Update the current estimates of the cluster centers by setting them to the mean of all instance belonging to that cluster  \n",
        "   \n",
        "   \n",
        "#### Objective function\n",
        "\n",
        "The underlying objective function tries to find cluster centers such that, if the data are partitioned into the corresponding clusters, distances between data points and their closest cluster centers become as small as possible.\n",
        "\n",
        "Given a set of datapoints ${x_1, ..., x_n}$ and a positive number $k$, find the clusters $C_1, ..., C_k$ that minimize\n",
        "\n",
        "\\begin{equation}\n",
        "J = \\sum_{i=1}^n \\, \\sum_{j=1}^k \\, z_{ij} \\, || x_i - \\mu_j ||_2\n",
        "\\end{equation}\n",
        "\n",
        "where:  \n",
        "- $z_{ij} \\in \\{0,1\\}$ defines whether of not datapoint $x_i$ belongs to cluster $C_j$\n",
        "- $\\mu_j$ denotes the cluster center of cluster $C_j$\n",
        "- $|| \\, ||_2$ denotes the Euclidean distance\n",
        "\n",
        "### Disadvantages of K-Means\n",
        "- The number of clusters has to be set in the beginning\n",
        "- The results depend on the inital cluster centers\n",
        "- It's sensitive to outliers\n",
        "- It's not guaranteed to find a global optimum, so it can get stuck in a local minimum\n",
        "\n",
        "\n",
        "###**2. SSPD DISTANCE (Symmetric Segment-Path Distance) [1]:**\n",
        "This is a modification of the Hausdorff distance. The authos shows a good behaviour of this distance when we are comparing trajectories (or poligonal curves).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  [1] P. Besse, B. Guillouet, J.-M. Loubes, and R. Francois, “Review and perspective for distance based trajectory clustering,” arXiv preprint arXiv:1508.04904, 2015."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dEAHCC7HUCZ",
        "colab_type": "text"
      },
      "source": [
        "# Es necesario instalar la libreria de distancia SSPD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59_o2WAoY86X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "ab3a308c-501f-43e1-c87d-484515589c92"
      },
      "source": [
        "%matplotlib inline\n",
        "!pip install traj_dist"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting traj_dist\n",
            "  Downloading https://files.pythonhosted.org/packages/44/f6/ce64756fa8335fb5b17435495aeb41491f8461ca2a9776ba87c12a30594a/traj_dist-1.15.tar.gz\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Processing /root/.cache/pip/wheels/69/63/0d/560a1741fa3f0ab897105cddb33f21f38c3330e4a57ea75db6/geohash2-1.1-cp36-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from traj_dist) (1.18.5)\n",
            "Requirement already satisfied: Shapely>=1.6.4 in /usr/local/lib/python3.6/dist-packages (from traj_dist) (1.7.0)\n",
            "Requirement already satisfied: pandas>=0.20.3 in /usr/local/lib/python3.6/dist-packages (from traj_dist) (1.0.5)\n",
            "Requirement already satisfied: Cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from traj_dist) (0.29.21)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from traj_dist) (1.4.1)\n",
            "Requirement already satisfied: docutils>=0.3 in /usr/local/lib/python3.6/dist-packages (from geohash2==1.1->traj_dist) (0.15.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.3->traj_dist) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.3->traj_dist) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.20.3->traj_dist) (1.15.0)\n",
            "Building wheels for collected packages: traj-dist\n",
            "  Building wheel for traj-dist (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for traj-dist: filename=traj_dist-1.15-cp36-cp36m-linux_x86_64.whl size=1496421 sha256=1a657899a6f80db2ac15dc6bf168d4b6134ba97b2c3a6518c304c8f2accbf1fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/da/29/cfefc74eb8b6a6a8b3a1ff2609c988a781d7a7d6c791da5d55\n",
            "Successfully built traj-dist\n",
            "Installing collected packages: geohash2, traj-dist\n",
            "Successfully installed geohash2-1.1 traj-dist-1.15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX6D1PzqY86m",
        "colab_type": "text"
      },
      "source": [
        "### Implementation\n",
        "We should have to implement kmeans with the new distance SSPD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Uwg3-uwY86-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %load kmeans.py\n",
        "import numpy as np\n",
        "import traj_dist.distance as tdist\n",
        "import numpy.matlib\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn import datasets\n",
        "from sklearn import metrics\n",
        "import scipy.io as spio\n",
        "import scipy.cluster.hierarchy as sch\n",
        "import pylab as pl\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from sklearn.metrics import pairwise_distances, silhouette_samples, silhouette_score\n",
        "\n",
        "\n",
        "class KMeans:\n",
        "    \n",
        "    def __init__(self, n_clusters=100):\n",
        "        self.K = n_clusters\n",
        "        \n",
        "    def fit(self, X):\n",
        "        #self.centroids = X[np.random.choice(len(X), self.K, replace=False),:]\n",
        "        self.centroids = X[np.random.choice(len(X), self.K),:]\n",
        "\n",
        "        self.intial_centroids = self.centroids\n",
        "        self.prev_label,  self.labels = None, np.zeros(len(X))\n",
        "        while not np.all(self.labels == self.prev_label) :\n",
        "            self.prev_label = self.labels\n",
        "            self.labels = self.predict(X)\n",
        "            self.update_centroid(X)\n",
        "        return self\n",
        "        \n",
        "    def predict(self, X):\n",
        "        #return np.apply_along_axis(self.compute_label, 1, X)\n",
        "        return np.apply_along_axis(self.compute_label, 1, X)\n",
        "\n",
        "\n",
        "    def compute_label(self, x):\n",
        "        k = len(self.centroids)\n",
        "        l=int(np.round(len(x)/2))\n",
        "        Tc=[]\n",
        "        for i in range(k):\n",
        "          ac=np.array(self.centroids[i,:l])\n",
        "          bc=np.array(self.centroids[i,l:])\n",
        "          traj=np.transpose(np.array([ac,bc]))\n",
        "          Tc.append(traj)\n",
        "        x2=numpy.matlib.repmat(x,k,1)\n",
        "        Tx=[]\n",
        "        for i in range(k):\n",
        "          ax2=np.array(x2[i,:l])\n",
        "          bx2=np.array(x2[i,l:])\n",
        "          traj=np.transpose(np.array([ax2,bx2]))\n",
        "          Tx.append(traj)\n",
        "        cdist = tdist.cdist(Tc, Tx, metric=\"sspd\")\n",
        "        ind = np.unravel_index(np.argmin(cdist, axis=None), cdist.shape)\n",
        "        return ind[0]\n",
        "        #return np.argmin(tdist.cdist(self.centroids, x, metric=\"sspd\"),axis=1)\n",
        "    def update_centroid(self, X):\n",
        "        self.centroids = np.array([np.mean(X[self.labels == k], axis=0)  for k in range(self.K)])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC2z6c9XY87G",
        "colab_type": "text"
      },
      "source": [
        "### DATA\n",
        "We use a database with polygonal curves extracted by discretization of lemniscates, ellipses and hyperbolas. It has 25079 curves.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQQ5fwOM_9ec",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "outputId": "7c109c1a-ff23-4373-e8bb-897a871b35e9"
      },
      "source": [
        "# Cargamos la base de datos (está en un repositorio en GitHub)\n",
        "!git  https://github.com/garestrear/poligonal_clusterization.git\n",
        "mat=spio.loadmat('poligonal_clusterization/DB_N_unif_08_2020.mat',squeeze_me=True)\n",
        "a=mat['curvasN'] # Es necesario convertirlos a tipo \"complex\" pues se cargan tipo str\n",
        "X_numpy=np.array(a,dtype = np.complex) # luego los conver a tipo \"array\"\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "git: 'https://github.com/garestrear/poligonal_clusterization' is not a git command. See 'git --help'.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'poligonal_clusterization/DB_N_unif_08_2020.mat'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a23f38289f38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Cargamos la base de datos (está en un repositorio en GitHub)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'git  https://github.com/garestrear/poligonal_clusterization'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'poligonal_clusterization/DB_N_unif_08_2020.mat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msqueeze_me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'curvasN'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Es necesario convertirlos a tipo \"complex\" pues se cargan tipo str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# luego los conver a tipo \"array\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \"\"\"\n\u001b[1;32m    215\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reader needs file name or open file-like object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'poligonal_clusterization/DB_N_unif_08_2020.mat'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMJPIPRV__Y-",
        "colab_type": "text"
      },
      "source": [
        "# Funciones adicionales\n",
        "\n",
        "\n",
        "*   Función ***dist_pol***: para calcular la distancia de una poligonal a un conjunto\n",
        "*   Función ***paint_cluster***: para pintar un cluster dado\n",
        "* Función para calclar la longitud de las poligonales y usarlas para controlar la \"semilla\" en la clusterización\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBsFT8IMfGJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Funciones creadas para este código\n",
        "\n",
        "# función que calcula la distancia de una poligonal a un conjunto de poligonales \n",
        "# almacenadas como las finals de un array\n",
        "def dist_pol(cpol1, MatPol):\n",
        "    '''# Esta función calcula la distancia entre una poligonal y una matriz de poligonales\n",
        "    úsela así [mean_dist, dist]=dist_pol(cpol1,MatPol) y obtenga la distancia promedio de \n",
        "    la poligonal a las demás poligonales del cluster y la distancia mínima '''\n",
        "    r=MatPol-cpol1\n",
        "    d2=np.apply_along_axis(np.linalg.norm,1,r)\n",
        "    dist=min(d2)\n",
        "    mean_dist=np.mean(d2)\n",
        "    return mean_dist, dist\n",
        "# Definimos un afunción que pinta un custer\n",
        "def paint_cluster(ncluster,clusters_pack):\n",
        "  '''# Use esta función así paint_cluster(ncluster,clusters_pack)\n",
        "  # recuerde que para obtener clusters_pack use\n",
        "  # clusters_pack= [X_numpy[labels==i] for i in range(cluster_number)]'''\n",
        "  X_numpy_C=clusters_pack[ncluster]\n",
        "  X=X_numpy_C.real\n",
        "  Y=X_numpy_C.imag\n",
        "  # Pintamos\n",
        "  plt.axis('equal')\n",
        "  plt.plot(np.transpose(X),np.transpose(Y))\n",
        "  plt.suptitle(('Cluster number %d ' % ncluster, len(X)),fontsize=28, fontweight='bold')\n",
        "\n",
        "def longitudes_pol(Mat_pol):\n",
        "    long_pol=list()\n",
        "    for pol in Mat_pol:\n",
        "        l=np.sum(np.abs(pol[0:-1]-pol[1:]))\n",
        "        long_pol.append(l)\n",
        "    return long_pol\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwszACdsKJoY",
        "colab_type": "text"
      },
      "source": [
        "# Representation of the Database\n",
        "We choose in an aleatory way some polygonals to paint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT5RgocFwUPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pintamos algunas de esas poligonales\n",
        "N_curvas2paint=400;\n",
        "a=np.random.randint(0,len(X_numpy), N_curvas2paint)\n",
        "b=np.sort(a)\n",
        "#print(b)\n",
        "longitudes=longitudes_pol(X_numpy)\n",
        "X=X_numpy.real\n",
        "Y=X_numpy.imag\n",
        "for k in b:\n",
        "  plt.plot(X[k,0:15],Y[k,0:15])\n",
        "  plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c_pTsf8GEyq",
        "colab_type": "text"
      },
      "source": [
        "# Reorganizamos las curvas según su longitud\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6djyp6JzGpP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "longitudes2=np.array(longitudes)\n",
        "longitudes3 =np.sort(longitudes2)\n",
        "sort_indices=np.argsort(longitudes2)\n",
        "X_numpy_sort=X_numpy[sort_indices,:]\n",
        "plt.hist(longitudes3, bins=1000)\n",
        "plt.xlim([0,2])\n",
        "print('La longitud máxima es: '+ str(np.max(longitudes3)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uns42WXSNMpZ",
        "colab_type": "text"
      },
      "source": [
        "Según el histograma anterior un gran número de poligonales tienen longitudes entre 1 y 1.5. Por lo anterior no es conveniente incluir las poligonales con mayor longitud en el proceso de clusterización. En palabras coloquiales estariamos introduciendo \"ruido\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LXYzKdyRqWg",
        "colab_type": "text"
      },
      "source": [
        "Clusterizamos el 95% de las curvas. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thJxY_QgRk5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a=0.95*len(X_numpy_sort)\n",
        "indice_final=int(np.floor(a))\n",
        "X_numpy_short=X_numpy_sort[range(indice_final),:]\n",
        "# Hacemos de nuevo el histograma\n",
        "longitudes4 =longitudes3[range(indice_final)]\n",
        "plt.hist(longitudes4, bins=1000)\n",
        "plt.ylim([0,800])\n",
        "print('La longitud máxima es: '+ str(np.max(longitudes4)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEzjtNrbKyTv",
        "colab_type": "text"
      },
      "source": [
        "# Clusterization\n",
        "Now we have the ingredients for the clusterization process. Lets make it real!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW_EoJPsw8Si",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Iniciamos el proceso de Clusterización\n",
        "np.random.seed(5)\n",
        "cluster_number=20\n",
        "X_numpy_r30_2=np.concatenate((X_numpy_sort.real, X_numpy_sort.imag),axis=1)\n",
        "poligonal_number=500\n",
        "X_numpy_r30=X_numpy_r30_2[0:poligonal_number,:]\n",
        "#longitudes=longitudes4[0:poligonal_number]\n",
        "#X=X_numpy_r30\n",
        "kmeans_model =KMeans(n_clusters=cluster_number) # estimators=[('k_means_iris_150', KMeans(n_clusters=150))]\n",
        "kmeans_model.fit(X_numpy_r30)\n",
        "labels = kmeans_model.labels\n",
        "#fignum = 1\n",
        "#titles = ['kmeans + SSPD with 150 custers']\n",
        "#estimators.fit(X)\n",
        "kmeans_Silhuette_coefficient=metrics.silhouette_score(X_numpy_r30,labels,metric='euclidean')\n",
        "\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjtfB6JPgcrU",
        "colab_type": "text"
      },
      "source": [
        "## **Cálculo de la cantidad de curvas en cada cluster y reorganización de la clustrización**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TckhNt24DPLp",
        "colab": {}
      },
      "source": [
        "labels2=labels\n",
        "# algoritmo para reorganización de clusters\n",
        "labels_aux=labels\n",
        "X_numpy2=X_numpy[0:poligonal_number,:]\n",
        "for k in range(cluster_number-1):\n",
        "  for j in range(k+1,cluster_number):\n",
        "    Numk=len(X_numpy2[labels_aux==k])\n",
        "    Numj=len(X_numpy2[labels_aux==j])\n",
        "    if Numk < Numj:\n",
        "      a=labels_aux==j\n",
        "      b=labels_aux==k\n",
        "      labels_aux[a]=k\n",
        "      labels_aux[b]=j\n",
        "         \n",
        "print(labels_aux)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kOw8jZ5u_he7"
      },
      "source": [
        "# Cluster Visualization. \n",
        "Use la función \n",
        "\n",
        " paint_cluster(ncluster,clusters_pack)\n",
        "\n",
        " Para pintar un cluster, \n",
        " Haga uso de la siguiente celda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzCcCOPVzsJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pylab import *\n",
        "clusters_pack2 = [X_numpy2[labels_aux==i] for i in range(cluster_number)]\n",
        "for f in range(cluster_number):\n",
        "  figure(f)\n",
        "  #ncluster=0 # Ponga el número del cluster que desea pintar\n",
        "  paint_cluster(f,clusters_pack2)\n",
        "show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLiE802G89HM",
        "colab_type": "text"
      },
      "source": [
        "# Silhuette analisis\n",
        "The Silhouette Coefficient is a measure of how well samples are clustered with samples that are similar to themselves. Clustering models with a high Silhouette Coefficient are said to be dense, where samples in the same cluster are similar to each other, and well separated, where samples in different clusters are not very similar to each other.\n",
        "\n",
        "The Silhouette Coefficient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. The Silhouette Coefficient for a sample is (b - a) / max(a, b). To clarify, b is the distance between a sample and the nearest cluster that the sample is not a part of. Note that Silhouette Coefficient is only defined if number of labels is 2 <= n_labels <= n_samples - 1.\n",
        "Explicación:\n",
        "Suponga que se ha realizado una clusterización sobre la base de datos.\n",
        "El silhouette coeff. (sc_i) se calcula para cada poligonal. Así:\n",
        "Suponga que tomamos la poligonal i-ésima y que está en el cluster #k (Ck)\n",
        "Paso 1. Se halla el promedio de las distancias de la poligonal i, a todas las poligonales contenidas en el mismo cluster (cluster Ck). Este número se denota por (a_i), i= desde 1 hasta #poligonales.\n",
        "\n",
        "Paso 2. Se halla la distancia promedio de la poligonal i a las poligonales del cluster Cj, con j diferente de k y luego se toma el mínimo de estas distancias promedios. Este número se denota (b_i)\n",
        "Observaciones.\n",
        "Se espera que b_i > a_i. Caso contrario la poligonalestá mal ubicada (Hay un cluster en el que encajaría mejor).\n",
        "El silhuette coeff (sc_i), es: (b_i-a_i)/max(b_i,a_i).\n",
        "El denominador max(b_i,a_i) es un factor de normalización.\n",
        "El caso ideal sería: todos los cluster cumplen que la distancia entre dos de sus elementos es siempre cero y por tanto a_i=0 y la distancia entre elementos de clusters diferentes es siempre mayor que cero. En este caso sc_i=1.\n",
        "El peor caso es cuando dos elementos iguales quedan en diferente cluster y por tanto b_i=0 pero la distancia entre elementos del mismo cluster no es cero así a_i>0. En este caso sc_i=-1.\n",
        "Finalmente el silhuette coeff. (sc) de la clusterización es (b-a)/max(a,b) donde b es promedio de los b_i y a es el promedio de los a_i.\n",
        "\n",
        "\n",
        "The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJSO8omYhzee",
        "colab_type": "text"
      },
      "source": [
        "## Silhuette coefficient for each cluster\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X6icLy9M9EH",
        "colab_type": "text"
      },
      "source": [
        "### Sihuette coefficients for kmeans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHW2WHO_jF_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Arriba se calculó\n",
        "#hc_Silhuette_coefficient=metrics.silhouette_score(X_numpy_r30, labels, metric='euclidean')\n",
        "kmeans_Silhuette_coefficient=metrics.silhouette_score(X_numpy_r30, labels_aux, metric='euclidean')\n",
        "\n",
        "# El anterior número da cuenta de la calidad de la clusterización\n",
        "silhouette_avg=kmeans_Silhuette_coefficient\n",
        "\n",
        "# Compute the silhouette scores for each sample\n",
        "sample_silhouette_values = silhouette_samples(X_numpy_r30, labels_aux)\n",
        "#print(sample_silhouette_values)\n",
        "fig, ax1 = plt.subplots(1, 1)\n",
        "fig.set_size_inches(36, 100)\n",
        "\n",
        "    # The 1st subplot is the silhouette plot\n",
        "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "ax1.set_xlim([-0.4, 1])\n",
        "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "space_cluster=75\n",
        "ax1.set_ylim([0, len(X_numpy2) + (cluster_number + 1) * space_cluster])\n",
        "y_lower = space_cluster\n",
        "color_option=['green', 'blue']\n",
        "for i in range(cluster_number):\n",
        "    # Aggregate the silhouette scores for samples belonging to\n",
        "    # cluster i, and sort them\n",
        "    ith_cluster_silhouette_values = \\\n",
        "      sample_silhouette_values[labels_aux == i]\n",
        "    \n",
        "    ith_cluster_silhouette_values.sort()\n",
        "\n",
        "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "    y_upper = y_lower + size_cluster_i\n",
        "    \n",
        "  \n",
        "    #color = cm.nipy_spectral(float(i) / cluster_number)  #Colormap type\n",
        "    color=color_option[i % 2]\n",
        "    ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                      0, ith_cluster_silhouette_values,\n",
        "                      facecolor=color, edgecolor=color, alpha=0.7)\n",
        "    # Pintamos también el promedio de los coeficientes silhuette en cada cluster\n",
        "    #ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "    mpc=np.mean(sample_silhouette_values[labels_aux == i]) #mean per cluster\n",
        "    #ax1.axvline(x=mpc, ymin=y_lower, ymax=y_lower+size_cluster_i, color=\"red\", linestyle=\"--\")\n",
        "    ax1.plot([mpc, mpc],[y_lower, y_lower+size_cluster_i], \\\n",
        "             color=\"green\", linestyle=\"--\",linewidth=4)\n",
        "   \n",
        "   # Label the silhouette plots with their cluster numbers at the middle\n",
        "    if size_cluster_i < 5:\n",
        "      if i % 2 == 1: # para escribir solo los impares\n",
        "        ax1.text(-0.2, y_lower + 0.5 * size_cluster_i, 'C'+str(i)+' with '+str(size_cluster_i),size=20)\n",
        "        ax1.plot([mpc],[y_lower],'og')\n",
        "      else:\n",
        "        ax1.plot([mpc],[y_lower],'og')\n",
        "    elif size_cluster_i < 10:\n",
        "      if i % 2 == 1: # para escribir solo los impares\n",
        "        ax1.text(-0.2, y_lower + 0.5 * size_cluster_i, 'C'+str(i)+' with '+str(size_cluster_i),size=20)\n",
        "        ax1.plot([mpc],[y_lower],'og')\n",
        "      else:\n",
        "        ax1.plot([mpc],[y_lower],'og')\n",
        "    elif size_cluster_i < 20:\n",
        "      if i % 2 == 1: # para escribir solo los impares\n",
        "        ax1.text(-0.2, y_lower + 0.5 * size_cluster_i, 'C'+str(i)+' with '+str(size_cluster_i),size=20)\n",
        "        ax1.plot([mpc],[y_lower],'og')\n",
        "      else:\n",
        "        ax1.plot([mpc],[y_lower],'og')\n",
        "    elif size_cluster_i < 50:\n",
        "      if i % 2 == 1: # para escribir solo los impares\n",
        "        ax1.text(-0.2, y_lower + 0.5 * size_cluster_i, 'C'+str(i)+' with '+str(size_cluster_i),size=20)\n",
        "        ax1.plot([mpc],[y_lower],'og')\n",
        "      else: \n",
        "        ax1.plot([mpc],[y_lower],'og') \n",
        "    elif size_cluster_i < 100:\n",
        "      ax1.text(-0.2, y_lower + 0.5 * size_cluster_i, 'C'+str(i)+' with '+str(size_cluster_i),size=20)\n",
        "    elif size_cluster_i < 500:\n",
        "      ax1.text(-0.2, y_lower + 0.5 * size_cluster_i, 'C'+str(i)+' with '+str(size_cluster_i),size=27)\n",
        "    elif size_cluster_i < 1000:\n",
        "      ax1.text(-0.2, y_lower + 0.5 * size_cluster_i, 'C'+str(i)+' with '+str(size_cluster_i),size=35)\n",
        "    else:\n",
        "      ax1.text(-0.2, y_lower + 0.5 * size_cluster_i, 'C'+str(i)+' with '+str(size_cluster_i),size=40)\n",
        "\n",
        "    # Compute the new y_lower for next plot\n",
        "    y_lower = y_upper + space_cluster  # 10 for the 0 samples\n",
        "\n",
        "ax1.set_title(\"The silhouette plot for the various clusters.\",fontsize=36)\n",
        "ax1.set_xlabel(\"The silhouette coefficient values\",fontsize=36)\n",
        "ax1.set_ylabel(\"Cluster label\",fontsize=36)\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "ax1.set_xticks([-0.4,-0.3,-0.2,-0.1, 0, 0.1,0.2, 0.3,0.4,0.5, 0.6, 0.7,0.8,\\\n",
        "                    0.9, 1])\n",
        "#plt.xticks(fontsize=16)\n",
        "plt.xticks(fontsize=34)\n",
        "plt.suptitle((\"Silhouette analysis for kmeans clusterization \"\n",
        "               \"with n_clusters = %d\" % cluster_number),\n",
        "              fontsize=28, fontweight='bold')\n",
        "\n",
        " #plt.show()\n",
        "   \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}